# -*- coding: utf-8 -*-
"""Crop_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UP-geXIx2o90xGQqW20NVFbzDhBMgN1x

#   <b>Multiclass Classification(Crop Dataset)

### <b>Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore")
import sqlite3
import pandas as pd
import numpy as np
# import nltk
import string
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.metrics import roc_curve, auc
# import re
import pickle
# from tqdm import tqdm
# import os
# from google.colab import files
# from google.colab import drive
# import io
from prettytable import PrettyTable
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import roc_auc_score
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import MinMaxScaler
from joblib import dump, load

# drive.mount('/content/drive')

# from google.colab import files

# uploaded = files.upload()

# import io

# data = pd.read_csv(io.BytesIO(uploaded['cpdata.csv']))



"""### <i>Above step required only if you are using colab"""

data=pd.read_csv("C:\\Users\\cheth\\Downloads\\cpdata.csv")

print("Number of data points in train data",data.shape)
print('-'*50)
print("The attributes of data :",data.columns.values)
data.head()

print(data['label'].value_counts())

data.dtypes

(data==np.nan).any()

"""### <i>We can see that the data set is completely balanced and there are no missing values. Hence, no futher interpolation or imputation of missing data required.

### <i>Let us convert the Labels into numbers for classification
"""

Type={}
for i,j in enumerate(data.label.unique()):
    Type[j]=i
print(Type)   
crop=[]
for i in data.label:
    crop.append(Type[i])
data['label']=crop

data.head()

"""# <b>Data Visualization"""

cols=list(data.columns.unique())
for col in cols[:-1]:
    sns.set_style("whitegrid")
    sns.boxplot('label',col, data=data)
    plt.xlabel("label") # Set text for the x axis
    plt.ylabel(col)# Set text for y axis
    plt.show()

"""### <i>We can see that the pH of various crops are similar and there inter quartile ranges overlap for most of them.Hence this would not a predictive feature on its own but the fact that their no much correlation of it with other features,can make it a potential feature when combined with other three.Therefore it is best to consider all the features for the model training.<br><br>It can also be observed from the box plots that there no outliers as such in the data.Hence ,no worries about the outliers.

# <b>Data Splitting

### <i>We split the data into Train data - used for training a model<br>Cross Validation data - used for tuning the hyper parameters<br>Test data - used for testing the trained model
"""

data=shuffle(data)
y=data['label']
X=data.drop(['label'],axis=1)
print("The shape of X:",X.shape)
print("The shape of y:",y.shape)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.18, stratify=y)
X_train,X_cv,y_train,y_cv=train_test_split(X_train,y_train,test_size=0.25,stratify=y_train)
print("The shape of train data:",'X:',X_train.shape,'y:',y_train.shape)
print("The shape of test data:",'X:',X_test.shape,'y:',y_test.shape)
print("The shape of cv data:",'X:',X_cv.shape,'y:',y_cv.shape)

"""# <b>Scaling the data

### <i>We use MinMax Scaling rather than  Z-score /Standard Scaling as most of them are far from <b>Normal</b> behaviour.
"""

scaler=MinMaxScaler()
scaler.fit(X_train)
X_train=scaler.transform(X_train)
X_test=scaler.transform(X_test)
X_cv=scaler.transform(X_cv)

"""# <b>Model Training

## <b>K Nearest Neighbors
"""

best_k=0
best_score=0
lb=LabelBinarizer()
lb.fit(y_cv)
binary=lb.transform(y_cv)
for k in range(5,61,5):
    clf = KNeighborsClassifier(n_neighbors=k)
    clf.fit(X_train,y_train)
    pred=clf.predict(X_cv)
    pred=lb.transform(pred)
    score=roc_auc_score(binary,pred)
    if best_score<=score:
        best_k=k
        best_score=score
print("The optimal value for k is:",best_k)

"""### <i>Train the final model with optimal parameter"""

knn =KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train,y_train)
pred=knn.predict(X_test)
lb.fit(y_test)
binary=lb.transform(y_test)
predF=lb.transform(pred)
score=roc_auc_score(binary,predF)
print("The auc is :",score)

"""### <i>Printing out some Actual and predicted Labels"""

ReturnType={v:k for k,v in Type.items()}
table= PrettyTable()
test=list(y_test)
pred=list(map(lambda a:ReturnType[a],pred[:500:25]))
test=list(map(lambda a:ReturnType[a],test[:500:25]))
table.add_column("Actual_label",test)
table.add_column("Predicted_label",pred)
print(table)

"""## <b>Support Vector Machine"""

best_C=0
best_score=0
lb=LabelBinarizer()
lb.fit(y_cv)
binary=lb.transform(y_cv)
for c in range(-5,6,1):
    clf = SVC(C=2**c,kernel='rbf',gamma='scale')
    clf.fit(X_train,y_train)
    pred=clf.predict(X_cv)
    pred=lb.transform(pred)
    score=roc_auc_score(binary,pred)
    if best_score<=score:
        best_C=c
        best_score=score
print("The optimal value for C is:",2**best_C)

svm =SVC(C=32,gamma='scale',kernel='rbf')
svm.fit(X_train,y_train)
pred=svm.predict(X_test)
lb.fit(y_test)
binary=lb.transform(y_test)
predF=lb.transform(pred)
score=roc_auc_score(binary,predF)
print("The auc is :",score)

table= PrettyTable()
test=list(y_test)
pred=list(map(lambda a:ReturnType[a],pred[:500:25]))
test=list(map(lambda a:ReturnType[a],test[:500:25]))
table.add_column("Actual_label",test)
table.add_column("Predicted_label",pred)
print(table)

"""## <b>Decision Tree """

best_depth=0
best_score=0
lb=LabelBinarizer()
lb.fit(y_cv)
binary=lb.transform(y_cv)
for d in range(2,21,2):
    clf = DecisionTreeClassifier(max_depth=d, random_state=3457)
    clf.fit(X_train,y_train)
    pred=clf.predict(X_cv)
    pred=lb.transform(pred)
    score=roc_auc_score(binary,pred)
    if best_score<=score:
        best_depth=d
        best_score=score
print("The optimal depth is:",best_depth)

DT = DecisionTreeClassifier(max_depth=16, random_state=3457)
DT.fit(X_train,y_train)
pred=DT.predict(X_test)
lb.fit(y_test)
binary=lb.transform(y_test)
predF=lb.transform(pred)
score=roc_auc_score(binary,predF)
print("The auc is :",score)

table= PrettyTable()
test=list(y_test)
pred=list(map(lambda a:ReturnType[a],pred[:500:25]))
test=list(map(lambda a:ReturnType[a],test[:500:25]))
table.add_column("Actual_label",test)
table.add_column("Predicted_label",pred)
print(table)

"""## <b>Random Forest Ensemble Model"""

best_depth=0
best_score=0
lb=LabelBinarizer()
lb.fit(y_cv)
binary=lb.transform(y_cv)
for d in range(2,21,2):
    clf = RandomForestClassifier(max_depth=d, random_state=3457,n_estimators=80)
    clf.fit(X_train,y_train)
    pred=clf.predict(X_cv)
    pred=lb.transform(pred)
    score=roc_auc_score(binary,pred)
    if best_score<=score:
        best_depth=d
        best_score=score
print("The optimal depth is:",best_depth)

RF= RandomForestClassifier(max_depth=20, random_state=3457,n_estimators=80)
RF.fit(X_train,y_train)
pred=RF.predict(X_test)
lb.fit(y_test)
binary=lb.transform(y_test)
predF=lb.transform(pred)
score=roc_auc_score(binary,predF)
print("The auc is :",score)

table= PrettyTable()
test=list(y_test)
pred=list(pred)
predT=list(map(lambda a:ReturnType[a],pred[:500:25]))
testT=list(map(lambda a:ReturnType[a],test[:500:25]))
table.add_column("Actual_label",testT)
table.add_column("Predicted_label",predT)
print(table)

"""## <b>Accuracy of individual CropType"""

from collections import defaultdict
predCount=defaultdict(int)
for i in range(len(pred)):
    if pred[i]==test[i]:
        predCount[pred[i]]+=1
for ele in data['label'].unique():
    predCount[ele]/=test.count(ele)
x=[]
y=[]
for i ,j in dict(predCount).items():
    x.append(ReturnType[i])
    y.append(j*100)

plt.figure(figsize=(40,20))
plt.bar(x,y)
plt.show()

table= PrettyTable()
model=x
AUC=y
table.add_column("Crop_Type",x)
table.add_column("Accuracy",list(map(lambda x:round(x,2),y)))
print(table)

"""## <b>Comparisons"""

table= PrettyTable()
model=['KNN','Support Vector Machine','Decision Tree','Random Forest']
AUC=[0.94259,0.96018,0.95555,0.96574]
table.add_column("Model",model)
table.add_column("AUC",AUC)
print(table)

"""# <b>Saving the model"""

pickle.dump(RF, open('C:\\Users\\cheth\\Desktop\\CropPredictRF.pkl', 'wb'))
pickle.dump(scaler, open('C:\\Users\\cheth\\Desktop\\CropScaler.pkl', 'wb'))

model=pickle.load(open('C:\\Users\\cheth\\Desktop\\CropScaler.pkl', 'rb'))
data={'temperature':[26],'humidity':[78],'ph':[5],'rainfall':[224]}
df=pd.DataFrame(data=data)
scaled_data=scaler.transform(df)
print("The scaled data",scaled_data)
print("Pickle is here")

"""### <i>The saved persisted model can be loaded whenever required<br>It is very important to Scale the new features incoming using the same Scaler which was used during the MinMax scaling.<br>1)Load both model and scaler<br>2)Transform new data by using transform method of scaler object<br>3)Use predict method of RFmodel object  """